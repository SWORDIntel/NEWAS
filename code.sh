#!/usr/bin/env python3
"""
Script to populate NEMWAS files with complete implementation code.
This script contains all the implementation code as strings and writes them to the appropriate files.
"""

import os
import sys
from pathlib import Path

# Color codes for output
GREEN = '\033[0;32m'
BLUE = '\033[0;34m'
YELLOW = '\033[1;33m'
RED = '\033[0;31m'
NC = '\033[0m'  # No Color

def create_file(filepath, content):
    """Create a file with the given content."""
    path = Path(filepath)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"  {GREEN}âœ“{NC} Created {filepath}")
        return True
    except Exception as e:
        print(f"  {RED}âœ—{NC} Failed to create {filepath}: {e}")
        return False

def main():
    """Main function to populate all NEMWAS files."""
    
    print(f"{BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{NC}")
    print(f"{BLUE}â•‘       NEMWAS - Complete Implementation Setup          â•‘{NC}")
    print(f"{BLUE}â•‘   This will populate all files with working code     â•‘{NC}")
    print(f"{BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}")
    print()
    
    # Check if we're in the right directory
    if not os.path.exists("src") or not os.path.exists("config"):
        print(f"{RED}Error: This script must be run from the NEMWAS root directory.{NC}")
        print("Please run: python populate_nemwas_files.py")
        sys.exit(1)
    
    print(f"{YELLOW}Starting file population...{NC}")
    print()
    
    # File content mappings
    files = {}
    
    # ===== CORE MODULE FILES =====
    
    files['src/core/npu_manager.py'] = '''"""NPU Manager for NEMWAS Framework - Debian Linux Compatible"""

import os
import logging
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import json
import time

import openvino as ov
import openvino.properties as props
import nncf
from nncf import QuantizationPreset

logger = logging.getLogger(__name__)


class NPUManager:
    """Manages NPU device detection, model optimization, and inference."""
    
    def __init__(self, cache_dir: str = "./models/cache"):
        self.core = ov.Core()
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Device detection
        self.available_devices = self._detect_devices()
        self.device_preference = self._set_device_preference()
        
        # Performance tracking
        self.device_metrics = {device: {"usage": 0, "memory": 0} for device in self.available_devices}
        
        logger.info(f"NPU Manager initialized. Available devices: {self.available_devices}")
        logger.info(f"Device preference order: {self.device_preference}")
        
    def _detect_devices(self) -> List[str]:
        """Detect available compute devices on Debian Linux."""
        devices = []
        
        try:
            # Get all available devices
            available = self.core.available_devices
            
            # Check for NPU (Intel Core Ultra)
            if "NPU" in available:
                devices.append("NPU")
                logger.info("NPU detected - Intel Neural Processing Unit available")
                
            # Check for GPU (Intel integrated or discrete)
            if "GPU" in available:
                devices.append("GPU")
                info = self._get_device_info("GPU")
                logger.info(f"GPU detected: {info}")
                
            # CPU is always available
            devices.append("CPU")
            
            # Check for MYRIAD (Neural Compute Stick 2)
            if "MYRIAD" in available:
                devices.append("MYRIAD")
                logger.info("Intel Neural Compute Stick 2 detected")
                
        except Exception as e:
            logger.warning(f"Error detecting devices: {e}")
            devices = ["CPU"]  # Fallback to CPU
            
        return devices
    
    def _set_device_preference(self) -> List[str]:
        """Set device preference order based on workload characteristics."""
        # NPU is preferred for INT8/INT4 inference
        # GPU for larger models with FP16
        # CPU as fallback
        preference = []
        
        if "NPU" in self.available_devices:
            preference.append("NPU")
        if "MYRIAD" in self.available_devices:
            preference.append("MYRIAD")
        if "GPU" in self.available_devices:
            preference.append("GPU")
        preference.append("CPU")
        
        return preference
    
    def _get_device_info(self, device: str) -> Dict[str, Any]:
        """Get detailed device information."""
        info = {}
        try:
            if device == "NPU":
                info["type"] = "Intel NPU"
                info["supported_properties"] = self.core.get_property(device, props.supported_properties)
                if hasattr(props.device, 'full_name'):
                    info["full_name"] = self.core.get_property(device, props.device.full_name)
            elif device == "GPU":
                info["type"] = "Intel GPU"
                if hasattr(props.device, 'full_name'):
                    info["full_name"] = self.core.get_property(device, props.device.full_name)
                info["memory"] = self._get_gpu_memory()
            elif device == "CPU":
                import cpuinfo
                cpu_info = cpuinfo.get_cpu_info()
                info["type"] = "CPU"
                info["brand"] = cpu_info.get('brand_raw', 'Unknown')
                info["cores"] = cpu_info.get('count', 0)
        except Exception as e:
            logger.warning(f"Could not get info for {device}: {e}")
            
        return info
    
    def _get_gpu_memory(self) -> int:
        """Get GPU memory on Linux."""
        try:
            # Try Intel GPU tools first
            import subprocess
            result = subprocess.run(['intel_gpu_top', '-l'], capture_output=True, text=True)
            if result.returncode == 0:
                # Parse memory from output
                for line in result.stdout.split('\\n'):
                    if 'memory' in line.lower():
                        # Extract memory value
                        pass
        except:
            pass
        return 0  # Default if can't determine
    
    def optimize_model_for_npu(self, 
                              model_path: str, 
                              model_type: str = "llm",
                              quantization_preset: str = "mixed") -> str:
        """Optimize model for NPU execution with quantization."""
        
        output_path = self.cache_dir / f"{Path(model_path).stem}_npu_optimized.xml"
        
        # Check if already optimized
        if output_path.exists():
            logger.info(f"Using cached optimized model: {output_path}")
            return str(output_path)
        
        logger.info(f"Optimizing model {model_path} for NPU...")
        
        try:
            # Load model
            if model_path.endswith('.xml'):
                model = self.core.read_model(model_path)
            else:
                # Convert from other formats (ONNX, etc)
                model = self._convert_to_openvino(model_path)
            
            # Apply NPU-specific optimizations
            if "NPU" in self.available_devices:
                # Quantize for NPU
                if quantization_preset == "performance":
                    preset = QuantizationPreset.PERFORMANCE
                elif quantization_preset == "mixed":
                    preset = QuantizationPreset.MIXED
                else:
                    preset = QuantizationPreset.DEFAULT
                
                # Create calibration dataset
                calibration_dataset = self._create_calibration_dataset(model_type)
                
                # Quantize model
                quantized_model = nncf.quantize(
                    model,
                    calibration_dataset,
                    preset=preset,
                    target_device=nncf.TargetDevice.NPU,
                    model_type=nncf.ModelType.TRANSFORMER if model_type == "llm" else None
                )
                
                # Save optimized model
                ov.save_model(quantized_model, output_path)
                logger.info(f"Model optimized and saved to: {output_path}")
                
            else:
                # Fallback optimization for CPU/GPU
                ov.save_model(model, output_path)
                
            return str(output_path)
            
        except Exception as e:
            logger.error(f"Failed to optimize model: {e}")
            raise
    
    def compile_model(self, 
                     model_path: str, 
                     device: Optional[str] = None,
                     config: Optional[Dict[str, Any]] = None) -> ov.CompiledModel:
        """Compile model for specific device with optimal settings."""
        
        if device is None:
            device = self.select_optimal_device(model_path)
            
        model = self.core.read_model(model_path)
        
        # Device-specific configurations
        device_config = self._get_device_config(device, model)
        if config:
            device_config.update(config)
            
        logger.info(f"Compiling model on {device} with config: {device_config}")
        
        try:
            compiled_model = self.core.compile_model(model, device, device_config)
            return compiled_model
        except Exception as e:
            logger.warning(f"Failed to compile on {device}: {e}")
            # Fallback to CPU
            if device != "CPU":
                logger.info("Falling back to CPU compilation")
                return self.core.compile_model(model, "CPU")
            raise
    
    def _get_device_config(self, device: str, model: ov.Model) -> Dict[str, Any]:
        """Get optimal configuration for device."""
        config = {}
        
        if device == "NPU":
            config = {
                props.cache_dir: str(self.cache_dir),
                props.performance_mode: props.PerformanceMode.LATENCY,
                props.inference_num_threads: 1,  # NPU handles threading internally
                "NPU_COMPILATION_MODE_PARAMS": "optimization-level=2",
                "NPU_TURBO": True,
            }
        elif device == "GPU":
            config = {
                props.cache_dir: str(self.cache_dir),
                props.performance_mode: props.PerformanceMode.THROUGHPUT,
                props.hint.num_requests: 2,
            }
        elif device == "CPU":
            config = {
                props.inference_num_threads: 4,
                props.affinity: props.Affinity.CORE,
                props.performance_mode: props.PerformanceMode.LATENCY,
                props.cache_dir: str(self.cache_dir),
            }
            
        return config
    
    def select_optimal_device(self, model_path: str) -> str:
        """Select optimal device based on model characteristics and device availability."""
        
        # Simple heuristic - can be enhanced with model profiling
        model_size = Path(model_path).stat().st_size / (1024 * 1024)  # MB
        
        for device in self.device_preference:
            if device == "NPU" and model_size < 4000:  # NPU good for models < 4GB
                return "NPU"
            elif device == "GPU" and model_size < 8000:  # GPU for medium models
                return "GPU"
            elif device == "CPU":
                return "CPU"
                
        return "CPU"  # Default fallback
    
    def benchmark_device(self, model_path: str, device: str, num_iterations: int = 10) -> Dict[str, float]:
        """Benchmark model on specific device."""
        logger.info(f"Benchmarking {model_path} on {device}...")
        
        try:
            compiled_model = self.compile_model(model_path, device)
            infer_request = compiled_model.create_infer_request()
            
            # Warmup
            for _ in range(3):
                infer_request.infer()
                
            # Benchmark
            times = []
            for _ in range(num_iterations):
                start = time.perf_counter()
                infer_request.infer()
                times.append(time.perf_counter() - start)
                
            return {
                "device": device,
                "mean_latency_ms": sum(times) / len(times) * 1000,
                "min_latency_ms": min(times) * 1000,
                "max_latency_ms": max(times) * 1000,
                "throughput_fps": 1.0 / (sum(times) / len(times))
            }
            
        except Exception as e:
            logger.error(f"Benchmark failed on {device}: {e}")
            return {"device": device, "error": str(e)}
    
    def _create_calibration_dataset(self, model_type: str):
        """Create calibration dataset for quantization."""
        # This is a simplified version - in production, use representative data
        import numpy as np
        
        if model_type == "llm":
            # Sample text sequences for LLM calibration
            return [
                {"input_ids": np.random.randint(0, 50000, size=(1, 128))},
                {"input_ids": np.random.randint(0, 50000, size=(1, 256))},
                {"input_ids": np.random.randint(0, 50000, size=(1, 512))},
            ] * 10
        else:
            # Generic calibration data
            return [
                {"input": np.random.randn(1, 3, 224, 224).astype(np.float32)}
                for _ in range(100)
            ]
    
    def _convert_to_openvino(self, model_path: str) -> ov.Model:
        """Convert non-OpenVINO models to OpenVINO IR format."""
        # Implement conversion logic for ONNX, PyTorch, etc.
        raise NotImplementedError("Model conversion not yet implemented")
    
    def get_device_metrics(self, device: str) -> Dict[str, Any]:
        """Get current device utilization metrics."""
        metrics = {"device": device}
        
        try:
            if device == "NPU":
                # NPU metrics would come from driver/system tools
                metrics["utilization"] = self._get_npu_utilization()
                metrics["memory_used"] = self._get_npu_memory()
            elif device == "CPU":
                import psutil
                metrics["utilization"] = psutil.cpu_percent()
                metrics["memory_used"] = psutil.virtual_memory().percent
        except Exception as e:
            logger.warning(f"Could not get metrics for {device}: {e}")
            
        return metrics
    
    def _get_npu_utilization(self) -> float:
        """Get NPU utilization on Linux."""
        # This would interface with Intel NPU driver tools
        # For now, return mock data
        return 0.0
    
    def _get_npu_memory(self) -> float:
        """Get NPU memory usage."""
        # This would interface with Intel NPU driver tools
        return 0.0
'''

    files['src/core/agent.py'] = '''"""Main NEMWAS Agent Implementation"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from pathlib import Path
import uuid

import openvino_genai as ov_genai
from pydantic import BaseModel, Field

from .npu_manager import NPUManager
from .react import ReActExecutor, Tool, ReActResult
from ..capability.learner import CapabilityLearner
from ..performance.tracker import PerformanceTracker
from ..utils.config import Config

logger = logging.getLogger(__name__)


@dataclass
class AgentContext:
    """Context for agent execution"""
    agent_id: str
    conversation_history: List[Dict[str, str]] = field(default_factory=list)
    current_task: Optional[str] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    capabilities: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class AgentConfig(BaseModel):
    """Agent configuration"""
    name: str = "NEMWAS-Agent"
    model_path: str
    device_preference: List[str] = ["NPU", "GPU", "CPU"]
    max_context_length: int = 4096
    temperature: float = 0.7
    max_iterations: int = 5
    enable_learning: bool = True
    enable_performance_tracking: bool = True


class NEMWASAgent:
    """Neural-Enhanced Multi-Workforce Agent System"""
    
    def __init__(self, config: AgentConfig, npu_manager: Optional[NPUManager] = None):
        self.config = config
        self.agent_id = str(uuid.uuid4())
        self.context = AgentContext(agent_id=self.agent_id)
        
        # Initialize NPU manager
        self.npu_manager = npu_manager or NPUManager()
        
        # Initialize LLM pipeline with NPU optimization
        self._initialize_llm()
        
        # Initialize components
        self.react_executor = ReActExecutor(self)
        self.capability_learner = CapabilityLearner() if config.enable_learning else None
        self.performance_tracker = PerformanceTracker() if config.enable_performance_tracking else None
        
        # Tool registry
        self.tools: Dict[str, Tool] = {}
        self._register_default_tools()
        
        logger.info(f"NEMWAS Agent {self.agent_id} initialized on device: {self.device}")
    
    def _initialize_llm(self):
        """Initialize LLM with NPU optimization"""
        try:
            # Check if model needs optimization
            model_path = Path(self.config.model_path)
            if not model_path.exists():
                raise FileNotFoundError(f"Model not found: {model_path}")
            
            # Optimize for NPU if available and not already optimized
            if self.npu_manager.available_devices[0] == "NPU" and not str(model_path).endswith("_npu_optimized.xml"):
                logger.info("Optimizing model for NPU...")
                optimized_path = self.npu_manager.optimize_model_for_npu(
                    str(model_path),
                    model_type="llm",
                    quantization_preset="mixed"
                )
                model_path = optimized_path
            
            # Select best device
            self.device = self.npu_manager.select_optimal_device(str(model_path))
            
            # Create LLM pipeline
            logger.info(f"Loading LLM pipeline on {self.device}...")
            self.llm_pipeline = ov_genai.LLMPipeline(str(model_path), self.device)
            
            # Configure generation parameters
            self.generation_config = ov_genai.GenerationConfig()
            self.generation_config.max_new_tokens = 512
            self.generation_config.temperature = self.config.temperature
            self.generation_config.do_sample = True
            self.generation_config.top_p = 0.9
            
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            raise
    
    def _register_default_tools(self):
        """Register default agent tools"""
        
        # Calculator tool
        self.register_tool(Tool(
            name="calculate",
            description="Perform mathematical calculations",
            function=self._calculate,
            parameters={"expression": "str"}
        ))
        
        # Memory tool
        self.register_tool(Tool(
            name="remember",
            description="Store information in agent memory",
            function=self._remember,
            parameters={"key": "str", "value": "str"}
        ))
        
        # Recall tool
        self.register_tool(Tool(
            name="recall",
            description="Retrieve information from agent memory",
            function=self._recall,
            parameters={"key": "str"}
        ))
    
    def register_tool(self, tool: Tool):
        """Register a new tool for the agent"""
        self.tools[tool.name] = tool
        logger.info(f"Registered tool: {tool.name}")
    
    async def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Process a query using ReAct pattern with NPU acceleration"""
        
        start_time = time.time()
        
        # Update context
        self.context.current_task = query
        self.context.conversation_history.append({"role": "user", "content": query})
        
        try:
            # Track performance
            if self.performance_tracker:
                self.performance_tracker.start_task(self.agent_id, query)
            
            # Execute ReAct loop
            result = await self.react_executor.execute(
                query=query,
                context=context,
                max_iterations=self.config.max_iterations
            )
            
            # Learn from execution if enabled
            if self.capability_learner and result.success:
                await self._learn_from_execution(query, result)
            
            # Update conversation history
            self.context.conversation_history.append({
                "role": "assistant", 
                "content": result.final_answer
            })
            
            # Track performance metrics
            if self.performance_tracker:
                execution_time = time.time() - start_time
                self.performance_tracker.end_task(
                    self.agent_id,
                    success=result.success,
                    execution_time=execution_time,
                    device_used=self.device,
                    iterations=len(result.steps)
                )
            
            return result.final_answer
            
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            error_msg = f"I encountered an error while processing your request: {str(e)}"
            self.context.conversation_history.append({
                "role": "assistant",
                "content": error_msg
            })
            return error_msg
    
    def generate(self, prompt: str) -> str:
        """Generate text using NPU-accelerated LLM"""
        
        # Add conversation context if available
        if self.context.conversation_history:
            context_prompt = self._build_context_prompt(prompt)
        else:
            context_prompt = prompt
        
        # Generate with NPU acceleration
        try:
            response = self.llm_pipeline.generate(context_prompt, self.generation_config)
            return response
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            # Fallback to simpler generation
            return "I apologize, but I'm having trouble generating a response right now."
    
    def _build_context_prompt(self, prompt: str) -> str:
        """Build prompt with conversation context"""
        
        # Limit context to fit within max length
        context_parts = []
        total_length = len(prompt)
        
        for msg in reversed(self.context.conversation_history[-10:]):  # Last 10 messages
            msg_text = f"{msg['role']}: {msg['content']}\\n"
            if total_length + len(msg_text) < self.config.max_context_length - 500:
                context_parts.insert(0, msg_text)
                total_length += len(msg_text)
            else:
                break
        
        if context_parts:
            context = "".join(context_parts)
            return f"Previous conversation:\\n{context}\\n\\nCurrent request: {prompt}"
        
        return prompt
    
    async def _learn_from_execution(self, query: str, result: ReActResult):
        """Learn from successful execution"""
        
        if not self.capability_learner:
            return
        
        # Extract successful patterns
        successful_tools = []
        for step in result.steps:
            if step.get("tool_name") and step.get("success"):
                successful_tools.append({
                    "tool": step["tool_name"],
                    "context": step.get("thought", ""),
                    "result": step.get("observation", "")
                })
        
        if successful_tools:
            # Create capability entry
            capability = {
                "query_pattern": query,
                "successful_tools": successful_tools,
                "execution_time": result.execution_time,
                "timestamp": time.time()
            }
            
            # Learn capability
            await self.capability_learner.learn(capability)
    
    # Default tool implementations
    def _calculate(self, expression: str) -> str:
        """Simple calculator tool"""
        try:
            # Safe evaluation of mathematical expressions
            import ast
            import operator
            
            # Allowed operators
            ops = {
                ast.Add: operator.add,
                ast.Sub: operator.sub,
                ast.Mult: operator.mul,
                ast.Div: operator.truediv,
                ast.Pow: operator.pow,
                ast.USub: operator.neg
            }
            
            def eval_expr(node):
                if isinstance(node, ast.Num):
                    return node.n
                elif isinstance(node, ast.BinOp):
                    return ops[type(node.op)](eval_expr(node.left), eval_expr(node.right))
                elif isinstance(node, ast.UnaryOp):
                    return ops[type(node.op)](eval_expr(node.operand))
                else:
                    raise TypeError(f"Unsupported type {type(node)}")
            
            node = ast.parse(expression, mode='eval')
            result = eval_expr(node.body)
            return f"The result is: {result}"
            
        except Exception as e:
            return f"Error calculating expression: {str(e)}"
    
    def _remember(self, key: str, value: str) -> str:
        """Store information in agent memory"""
        if "memory" not in self.context.metadata:
            self.context.metadata["memory"] = {}
        
        self.context.metadata["memory"][key] = value
        return f"Stored '{key}' in memory"
    
    def _recall(self, key: str) -> str:
        """Retrieve information from agent memory"""
        memory = self.context.metadata.get("memory", {})
        
        if key in memory:
            return f"Retrieved from memory: {memory[key]}"
        else:
            return f"No memory found for key '{key}'"
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get agent performance metrics"""
        metrics = {
            "agent_id": self.agent_id,
            "device": self.device,
            "total_queries": len(self.context.conversation_history) // 2,
            "capabilities": len(self.context.capabilities)
        }
        
        if self.performance_tracker:
            metrics.update(self.performance_tracker.get_agent_metrics(self.agent_id))
        
        # Add NPU metrics
        if self.device == "NPU":
            metrics["npu_metrics"] = self.npu_manager.get_device_metrics("NPU")
        
        return metrics
    
    def export_context(self) -> Dict[str, Any]:
        """Export agent context for persistence"""
        return {
            "agent_id": self.agent_id,
            "conversation_history": self.context.conversation_history,
            "capabilities": self.context.capabilities,
            "metadata": self.context.metadata,
            "performance_metrics": self.context.performance_metrics
        }
    
    def import_context(self, context_data: Dict[str, Any]):
        """Import agent context from saved data"""
        self.context.conversation_history = context_data.get("conversation_history", [])
        self.context.capabilities = context_data.get("capabilities", [])
        self.context.metadata = context_data.get("metadata", {})
        self.context.performance_metrics = context_data.get("performance_metrics", {})
'''

    files['src/core/react.py'] = '''"""ReAct (Reasoning and Acting) Pattern Implementation"""

import re
import json
import logging
import asyncio
import time
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class ActionType(Enum):
    """Types of actions in ReAct pattern"""
    THINK = "think"
    ACT = "act"
    OBSERVE = "observe"
    ANSWER = "answer"


@dataclass
class Tool:
    """Tool definition for agent use"""
    name: str
    description: str
    function: Callable
    parameters: Dict[str, str]  # parameter_name -> type_description
    
    async def execute(self, **kwargs) -> str:
        """Execute tool with given parameters"""
        try:
            if asyncio.iscoroutinefunction(self.function):
                result = await self.function(**kwargs)
            else:
                result = self.function(**kwargs)
            return str(result)
        except Exception as e:
            logger.error(f"Tool {self.name} execution failed: {e}")
            return f"Error: {str(e)}"


@dataclass
class ReActStep:
    """Single step in ReAct execution"""
    step_number: int
    action_type: ActionType
    content: str
    tool_name: Optional[str] = None
    tool_params: Optional[Dict[str, Any]] = None
    observation: Optional[str] = None
    timestamp: float = 0.0


@dataclass
class ReActResult:
    """Result of ReAct execution"""
    success: bool
    final_answer: str
    steps: List[Dict[str, Any]]
    execution_time: float
    iterations_used: int


class ReActExecutor:
    """Executes ReAct pattern for problem solving"""
    
    # ReAct prompt template
    REACT_TEMPLATE = """You are an AI assistant that solves problems step by step using the ReAct (Reasoning and Acting) approach.

Available tools:
{tools_description}

Format your response EXACTLY as follows:
Thought: [Your reasoning about what to do next]
Action: [tool_name]
Action Input: {{"param1": "value1", "param2": "value2"}}

After receiving an observation, continue with:
Thought: [Reflection on the observation]
...

When you have the final answer:
Thought: [Final reasoning]
Answer: [Your final answer to the user]

Question: {query}

Let's solve this step by step."""

    def __init__(self, agent):
        self.agent = agent
        self.tools = agent.tools
        
    async def execute(self, 
                     query: str, 
                     context: Optional[Dict[str, Any]] = None,
                     max_iterations: int = 5) -> ReActResult:
        """Execute ReAct loop to solve a problem"""
        
        start_time = time.time()
        steps = []
        iteration = 0
        
        # Build tools description
        tools_desc = self._build_tools_description()
        
        # Initial prompt
        prompt = self.REACT_TEMPLATE.format(
            tools_description=tools_desc,
            query=query
        )
        
        # Add context if provided
        if context:
            prompt += f"\\n\\nAdditional context: {json.dumps(context)}"
        
        # Conversation history for multi-turn reasoning
        conversation = prompt
        
        while iteration < max_iterations:
            iteration += 1
            
            try:
                # Generate next action
                response = self.agent.generate(conversation)
                
                # Parse response
                action_type, content, tool_info = self._parse_response(response)
                
                # Create step record
                step = ReActStep(
                    step_number=iteration,
                    action_type=action_type,
                    content=content,
                    timestamp=time.time()
                )
                
                # Handle different action types
                if action_type == ActionType.ANSWER:
                    # Final answer reached
                    return ReActResult(
                        success=True,
                        final_answer=content,
                        steps=self._steps_to_dict(steps + [step]),
                        execution_time=time.time() - start_time,
                        iterations_used=iteration
                    )
                
                elif action_type == ActionType.ACT and tool_info:
                    # Execute tool
                    tool_name, tool_params = tool_info
                    step.tool_name = tool_name
                    step.tool_params = tool_params
                    
                    if tool_name in self.tools:
                        observation = await self.tools[tool_name].execute(**tool_params)
                        step.observation = observation
                        
                        # Add observation to conversation
                        conversation += f"\\n{response}\\nObservation: {observation}\\n"
                    else:
                        # Tool not found
                        observation = f"Error: Tool '{tool_name}' not found"
                        step.observation = observation
                        conversation += f"\\n{response}\\nObservation: {observation}\\n"
                
                else:
                    # Just thinking, continue conversation
                    conversation += f"\\n{response}\\n"
                
                steps.append(step)
                
            except Exception as e:
                logger.error(f"ReAct execution error at iteration {iteration}: {e}")
                # Try to recover or fail gracefully
                return ReActResult(
                    success=False,
                    final_answer=f"I encountered an error while solving this problem: {str(e)}",
                    steps=self._steps_to_dict(steps),
                    execution_time=time.time() - start_time,
                    iterations_used=iteration
                )
        
        # Max iterations reached
        return ReActResult(
            success=False,
            final_answer="I couldn't complete the task within the allowed iterations. Here's what I found so far: " + 
                         self._summarize_steps(steps),
            steps=self._steps_to_dict(steps),
            execution_time=time.time() - start_time,
            iterations_used=iteration
        )
    
    def _build_tools_description(self) -> str:
        """Build formatted description of available tools"""
        descriptions = []
        
        for name, tool in self.tools.items():
            params_str = ", ".join([f"{p}: {t}" for p, t in tool.parameters.items()])
            descriptions.append(f"- {name}({params_str}): {tool.description}")
        
        return "\\n".join(descriptions)
    
    def _parse_response(self, response: str) -> Tuple[ActionType, str, Optional[Tuple[str, Dict]]]:
        """Parse model response to extract action type and content"""
        
        # Clean response
        response = response.strip()
        
        # Check for final answer
        answer_match = re.search(r'Answer:\\s*(.+?)(?:\\n|$)', response, re.IGNORECASE | re.DOTALL)
        if answer_match:
            return ActionType.ANSWER, answer_match.group(1).strip(), None
        
        # Check for action
        action_match = re.search(r'Action:\\s*(\\w+)', response, re.IGNORECASE)
        if action_match:
            tool_name = action_match.group(1).strip()
            
            # Extract action input
            input_match = re.search(r'Action Input:\\s*({.+?})', response, re.IGNORECASE | re.DOTALL)
            if input_match:
                try:
                    tool_params = json.loads(input_match.group(1))
                    return ActionType.ACT, response, (tool_name, tool_params)
                except json.JSONDecodeError:
                    logger.warning(f"Failed to parse action input: {input_match.group(1)}")
                    # Try to extract parameters manually
                    tool_params = self._extract_params_fallback(input_match.group(1))
                    if tool_params:
                        return ActionType.ACT, response, (tool_name, tool_params)
        
        # Check for thought
        thought_match = re.search(r'Thought:\\s*(.+?)(?:\\n|$)', response, re.IGNORECASE | re.DOTALL)
        if thought_match:
            return ActionType.THINK, thought_match.group(1).strip(), None
        
        # Default to thinking if no clear pattern
        return ActionType.THINK, response, None
    
    def _extract_params_fallback(self, params_str: str) -> Optional[Dict[str, Any]]:
        """Fallback parameter extraction for malformed JSON"""
        try:
            # Try to fix common JSON errors
            params_str = params_str.strip()
            
            # Add quotes to unquoted keys
            params_str = re.sub(r'(\\w+):', r'"\\1":', params_str)
            
            # Try parsing again
            return json.loads(params_str)
        except:
            # Manual extraction as last resort
            params = {}
            matches = re.findall(r'["\']?(\\w+)["\']?\\s*:\\s*["\']?([^,"\\']+)["\']?', params_str)
            for key, value in matches:
                # Try to infer type
                if value.lower() in ['true', 'false']:
                    params[key] = value.lower() == 'true'
                elif value.replace('.', '').replace('-', '').isdigit():
                    params[key] = float(value) if '.' in value else int(value)
                else:
                    params[key] = value.strip()
            
            return params if params else None
    
    def _steps_to_dict(self, steps: List[ReActStep]) -> List[Dict[str, Any]]:
        """Convert ReActStep objects to dictionaries"""
        return [
            {
                "step_number": step.step_number,
                "action_type": step.action_type.value,
                "content": step.content,
                "tool_name": step.tool_name,
                "tool_params": step.tool_params,
                "observation": step.observation,
                "timestamp": step.timestamp
            }
            for step in steps
        ]
    
    def _summarize_steps(self, steps: List[ReActStep]) -> str:
        """Summarize execution steps for partial results"""
        summary = []
        
        for step in steps:
            if step.action_type == ActionType.ACT and step.observation:
                summary.append(f"- Used {step.tool_name}: {step.observation}")
            elif step.action_type == ActionType.THINK:
                summary.append(f"- Reasoning: {step.content[:100]}...")
        
        return "\\n".join(summary) if summary else "No significant progress made."
'''

    # Continue with more files...
    files['main.py'] = '''#!/usr/bin/env python3
"""NEMWAS - Neural-Enhanced Multi-Workforce Agent System"""

import asyncio
import signal
import sys
import logging
from pathlib import Path
import click
import uvicorn
from rich.console import Console
from rich.logging import RichHandler

# NEMWAS imports
from src.core.agent import NEMWASAgent, AgentConfig
from src.core.npu_manager import NPUManager
from src.nlp.interface import NaturalLanguageInterface
from src.plugins.interface import PluginRegistry
from src.performance.tracker import PerformanceTracker
from src.utils.config import load_config
from src.api.server import create_app

# Setup logging
console = Console()
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(console=console, rich_tracebacks=True)]
)
logger = logging.getLogger("nemwas")


class NEMWASCore:
    """Core NEMWAS system orchestrator"""
    
    def __init__(self, config_path: str):
        self.config = load_config(config_path)
        self.running = False
        
        # Initialize components
        logger.info("ðŸš€ Initializing NEMWAS Core...")
        
        # NPU Manager
        self.npu_manager = NPUManager(cache_dir=self.config.get('cache_dir', './models/cache'))
        
        # Performance Tracker
        self.performance_tracker = PerformanceTracker(
            metrics_dir=self.config.get('metrics_dir', './data/metrics'),
            enable_prometheus=self.config.get('enable_prometheus', True)
        )
        
        # Plugin Registry
        self.plugin_registry = PluginRegistry(
            plugin_dirs=self.config.get('plugin_dirs', ['./plugins/builtin', './plugins/community'])
        )
        self.plugin_registry.set_npu_manager(self.npu_manager)
        
        # Natural Language Interface
        self.nl_interface = NaturalLanguageInterface()
        
        # Agent pool
        self.agents = {}
        
        # Create default agent
        self._create_default_agent()
        
        logger.info("âœ… NEMWAS Core initialized successfully")
    
    def _create_default_agent(self):
        """Create the default agent"""
        
        agent_config = AgentConfig(
            name="Default-Agent",
            model_path=self.config['default_model_path'],
            device_preference=["NPU", "GPU", "CPU"],
            enable_learning=True,
            enable_performance_tracking=True
        )
        
        try:
            agent = NEMWASAgent(agent_config, self.npu_manager)
            self.agents[agent.agent_id] = agent
            logger.info(f"Created default agent: {agent.agent_id}")
        except Exception as e:
            logger.error(f"Failed to create default agent: {e}")
    
    async def process_command(self, command: str) -> str:
        """Process a natural language command"""
        
        # Parse intent
        intent = self.nl_interface.parse(command)
        logger.debug(f"Parsed intent: {intent.intent_type} (confidence: {intent.confidence:.2f})")
        
        # Route to appropriate handler
        if intent.intent_type.value == "execute_task":
            # Use default agent for task execution
            if self.agents:
                agent = list(self.agents.values())[0]
                result = await agent.process(intent.entities.get('task', command))
                return result
            else:
                return "No agents available. Please create an agent first."
        
        elif intent.intent_type.value == "create_agent":
            # Create new agent
            return await self._handle_create_agent(intent)
        
        elif intent.intent_type.value == "query_status":
            # Get system status
            return self._handle_status_query(intent)
        
        elif intent.intent_type.value == "analyze_performance":
            # Analyze performance
            return self._handle_performance_analysis(intent)
        
        else:
            # Default to using the agent
            if self.agents:
                agent = list(self.agents.values())[0]
                result = await agent.process(command)
                return result
            else:
                return "I'm not sure how to handle that request. Try 'help' for available commands."
    
    async def _handle_create_agent(self, intent):
        """Handle agent creation request"""
        
        try:
            # Extract configuration from intent
            purpose = intent.entities.get('purpose', 'general')
            
            agent_config = AgentConfig(
                name=f"Agent-{purpose[:20]}",
                model_path=self.config['default_model_path'],
                device_preference=["NPU", "GPU", "CPU"],
                enable_learning=True,
                enable_performance_tracking=True
            )
            
            agent = NEMWASAgent(agent_config, self.npu_manager)
            self.agents[agent.agent_id] = agent
            
            return f"Successfully created agent {agent.agent_id} for {purpose}"
            
        except Exception as e:
            logger.error(f"Failed to create agent: {e}")
            return f"Failed to create agent: {str(e)}"
    
    def _handle_status_query(self, intent):
        """Handle status query"""
        
        status = {
            'system': self.performance_tracker.get_system_metrics(),
            'agents': len(self.agents),
            'active_agents': sum(1 for a in self.agents.values() if a.context.current_task),
            'npu_available': "NPU" in self.npu_manager.available_devices,
            'plugins_loaded': len(self.plugin_registry.plugins)
        }
        
        return self.nl_interface.generate_response(intent, {'system': status})
    
    def _handle_performance_analysis(self, intent):
        """Handle performance analysis request"""
        
        agent_id = intent.entities.get('agent_id')
        
        if agent_id and agent_id in self.agents:
            analysis = self.performance_tracker.analyze_performance_trends(agent_id)
        else:
            analysis = self.performance_tracker.analyze_performance_trends()
        
        return self.nl_interface.generate_response(intent, analysis)
    
    async def start(self):
        """Start the NEMWAS system"""
        
        self.running = True
        logger.info("ðŸš€ Starting NEMWAS system...")
        
        # Load plugins
        self._load_plugins()
        
        # Start API server
        app = create_app(self)
        
        # Setup signal handlers
        def signal_handler(sig, frame):
            logger.info("Received shutdown signal...")
            self.stop()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Run server
        config = uvicorn.Config(
            app=app,
            host=self.config.get('api_host', '0.0.0.0'),
            port=self.config.get('api_port', 8080),
            log_level="info"
        )
        server = uvicorn.Server(config)
        
        await server.serve()
    
    def _load_plugins(self):
        """Load configured plugins"""
        
        logger.info("Loading plugins...")
        
        # Discover available plugins
        available = self.plugin_registry.discover_plugins()
        logger.info(f"Found {len(available)} available plugins")
        
        # Load configured plugins
        for plugin_path in self.config.get('plugins', []):
            if self.plugin_registry.load_plugin(plugin_path):
                logger.info(f"âœ“ Loaded plugin: {plugin_path}")
            else:
                logger.warning(f"âœ— Failed to load plugin: {plugin_path}")
        
        # Register plugin tools with agents
        tools = self.plugin_registry.get_tools()
        for agent in self.agents.values():
            for tool_name, tool_def in tools.items():
                # Create tool wrapper
                from src.core.react import Tool
                
                plugin_tool = Tool(
                    name=tool_name,
                    description=tool_def['description'],
                    function=tool_def['function'],
                    parameters=tool_def['parameters']
                )
                
                agent.register_tool(plugin_tool)
    
    def stop(self):
        """Stop the NEMWAS system"""
        
        logger.info("Shutting down NEMWAS...")
        self.running = False
        
        # Export metrics
        self.performance_tracker.export_metrics()
        
        # Cleanup agents
        for agent in self.agents.values():
            agent.export_context()
        
        logger.info("NEMWAS shutdown complete")


@click.command()
@click.option('--config', '-c', default='config/default.yaml', help='Configuration file path')
@click.option('--interactive', '-i', is_flag=True, help='Start in interactive mode')
@click.option('--command', help='Execute a single command and exit')
def main(config, interactive, command):
    """NEMWAS - Neural-Enhanced Multi-Workforce Agent System"""
    
    console.print("[bold blue]NEMWAS v1.0[/bold blue] - Neural-Enhanced Multi-Workforce Agent System")
    console.print("=" * 60)
    
    # Create core system
    core = NEMWASCore(config)
    
    if command:
        # Execute single command
        async def run_command():
            result = await core.process_command(command)
            console.print(result)
        
        asyncio.run(run_command())
        
    elif interactive:
        # Interactive mode
        console.print("Interactive mode. Type 'help' for commands, 'exit' to quit.")
        console.print()
        
        async def interactive_loop():
            while True:
                try:
                    user_input = console.input("[bold green]nemwas>[/bold green] ")
                    
                    if user_input.lower() in ['exit', 'quit']:
                        break
                    
                    result = await core.process_command(user_input)
                    console.print(result)
                    console.print()
                    
                except KeyboardInterrupt:
                    break
                except Exception as e:
                    console.print(f"[red]Error: {e}[/red]")
        
        asyncio.run(interactive_loop())
        
    else:
        # Server mode
        try:
            asyncio.run(core.start())
        except KeyboardInterrupt:
            pass
    
    core.stop()


if __name__ == "__main__":
    main()
'''

    # Create remaining essential files (truncated for space - you get the pattern)
    files['src/capability/learner.py'] = '''"""Capability Learning Engine - Placeholder"""
import logging
logger = logging.getLogger(__name__)

class CapabilityLearner:
    def __init__(self):
        logger.info("CapabilityLearner initialized")
    
    async def learn(self, capability_data):
        logger.info(f"Learning from: {capability_data}")
        return True
'''

    files['src/performance/tracker.py'] = '''"""Performance Tracker - Placeholder"""
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class PerformanceTracker:
    def __init__(self, metrics_dir="./data/metrics", enable_prometheus=True):
        self.metrics_dir = metrics_dir
        logger.info("PerformanceTracker initialized")
    
    def start_task(self, agent_id: str, query: str):
        logger.info(f"Starting task for agent {agent_id}")
    
    def end_task(self, agent_id: str, success: bool, execution_time: float, device_used: str, iterations: int):
        logger.info(f"Task completed for agent {agent_id}")
    
    def get_agent_metrics(self, agent_id: str) -> Dict[str, Any]:
        return {"agent_id": agent_id, "metrics": {}}
    
    def get_system_metrics(self) -> Dict[str, Any]:
        return {"system": "running"}
    
    def analyze_performance_trends(self, agent_id=None):
        return {"trend": "stable"}
    
    def export_metrics(self):
        logger.info("Exporting metrics")
'''

    files['src/nlp/interface.py'] = '''"""Natural Language Interface - Placeholder"""
import logging
from enum import Enum
from dataclasses import dataclass
from typing import Dict, Any

logger = logging.getLogger(__name__)

class IntentType(Enum):
    EXECUTE_TASK = "execute_task"
    CREATE_AGENT = "create_agent"
    QUERY_STATUS = "query_status"
    ANALYZE_PERFORMANCE = "analyze_performance"
    HELP = "help"

@dataclass
class ParsedIntent:
    intent_type: IntentType
    confidence: float
    entities: Dict[str, Any]
    original_text: str

class NaturalLanguageInterface:
    def __init__(self):
        logger.info("NaturalLanguageInterface initialized")
    
    def parse(self, text: str) -> ParsedIntent:
        # Simple intent detection
        text_lower = text.lower()
        
        if "create" in text_lower and "agent" in text_lower:
            intent_type = IntentType.CREATE_AGENT
        elif "status" in text_lower:
            intent_type = IntentType.QUERY_STATUS
        elif "performance" in text_lower:
            intent_type = IntentType.ANALYZE_PERFORMANCE
        elif "help" in text_lower:
            intent_type = IntentType.HELP
        else:
            intent_type = IntentType.EXECUTE_TASK
        
        return ParsedIntent(
            intent_type=intent_type,
            confidence=0.8,
            entities={"task": text},
            original_text=text
        )
    
    def generate_response(self, intent: ParsedIntent, result: Any) -> str:
        return str(result)
'''

    files['src/plugins/interface.py'] = '''"""Plugin Interface - Placeholder"""
import logging
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class PluginRegistry:
    def __init__(self, plugin_dirs=None):
        self.plugin_dirs = plugin_dirs or []
        self.plugins = {}
        logger.info("PluginRegistry initialized")
    
    def set_npu_manager(self, npu_manager):
        self.npu_manager = npu_manager
    
    def discover_plugins(self) -> List[str]:
        return []
    
    def load_plugin(self, plugin_path: str) -> bool:
        logger.info(f"Loading plugin: {plugin_path}")
        return False
    
    def get_tools(self) -> Dict[str, Any]:
        return {}
'''

    files['src/utils/config.py'] = '''"""Configuration utilities"""
import yaml
import json
import logging
from pathlib import Path
from typing import Dict, Any

logger = logging.getLogger(__name__)

def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from file"""
    path = Path(config_path)
    
    if not path.exists():
        logger.warning(f"Config file not found: {config_path}, using defaults")
        return {
            "default_model_path": "./models/original/tinyllama-1.1b-chat.xml",
            "cache_dir": "./models/cache",
            "metrics_dir": "./data/metrics",
            "enable_prometheus": True,
            "plugin_dirs": ["./plugins/builtin", "./plugins/community"],
            "plugins": [],
            "api_host": "0.0.0.0",
            "api_port": 8080
        }
    
    try:
        with open(path, 'r') as f:
            if path.suffix in ['.yaml', '.yml']:
                config = yaml.safe_load(f)
            else:
                config = json.load(f)
        
        # Flatten nested config
        flat_config = {}
        for section, values in config.items():
            if isinstance(values, dict):
                flat_config.update(values)
            else:
                flat_config[section] = values
        
        return flat_config
        
    except Exception as e:
        logger.error(f"Failed to load config: {e}")
        return {}
'''

    files['src/api/server.py'] = '''"""FastAPI Server - Placeholder"""
from fastapi import FastAPI
import logging

logger = logging.getLogger(__name__)

def create_app(nemwas_core):
    """Create FastAPI application"""
    app = FastAPI(title="NEMWAS API", version="1.0.0")
    
    # Store core reference
    app.state.core = nemwas_core
    
    @app.get("/")
    async def root():
        return {"service": "NEMWAS API", "version": "1.0.0", "status": "running"}
    
    @app.get("/health")
    async def health():
        return {"status": "healthy"}
    
    return app
'''

    # Process all files
    success_count = 0
    fail_count = 0
    
    print(f"{BLUE}Creating implementation files...{NC}")
    for filepath, content in files.items():
        if create_file(filepath, content):
            success_count += 1
        else:
            fail_count += 1
    
    # Summary
    print()
    print(f"{BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}")
    print(f"{GREEN}âœ… File population complete!{NC}")
    print(f"{BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}")
    print(f"  Files created: {success_count}")
    if fail_count > 0:
        print(f"  Files failed: {fail_count}")
    print()
    print(f"{YELLOW}Note: This script includes simplified versions of some modules.{NC}")
    print(f"{YELLOW}For the complete implementation, copy from the provided artifacts.{NC}")
    print()
    print("Next steps:")
    print("1. Run: pip install -r requirements.txt")
    print("2. Run: python scripts/download_models.py --minimal")
    print("3. Run: python main.py --interactive")

if __name__ == "__main__":
    main()
'''

    # Create both scripts
    success = 0
    
    # Create the structure setup script
    if create_file("setup_nemwas_structure.sh", files['setup_nemwas_structure.sh']):
        os.chmod("setup_nemwas_structure.sh", 0o755)
        print(f"  {GREEN}âœ“{NC} Created setup_nemwas_structure.sh (executable)")
        success += 1
    
    # Create the file population script
    if create_file("populate_nemwas_files.py", files['populate_nemwas_files.py']):
        os.chmod("populate_nemwas_files.py", 0o755)
        print(f"  {GREEN}âœ“{NC} Created populate_nemwas_files.py (executable)")
        success += 1
    
    if success == 2:
        print(f"\n{GREEN}âœ… Both setup scripts created successfully!{NC}")
        print("\nTo set up NEMWAS:")
        print("1. Run: ./setup_nemwas_structure.sh")
        print("2. Run: python populate_nemwas_files.py")
        print("3. Follow the quickstart instructions")
    else:
        print(f"\n{RED}Failed to create some scripts{NC}")
